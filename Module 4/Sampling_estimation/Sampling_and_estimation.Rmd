---
title: "Inferential Statistics: Sampling & Estimation"
output: html_notebook
---

Inferential statistics involves two major componenets - Estimation and Hypothesis Testing. This notebook will cover 'Estimation'. 

#### Revisiting Sampling theory

Before starting with Estimation, we need to understand the sampling theory, a part of which we've also covered in the first module.

It is necessary to understand what we're drawing inferences from (*sample*) and what we're drawing inferences about (*population*).
As we've previously discussed, **Population** refers to the set of all possible people, or all possible observations, that you want to draw conclusions about, and is generally much bigger than the sample.

If I am a cognitive scientist who wants to stude=y about the brain, what should I count as my population of interest:
- All undergraduate students in Psychology at IITK?
- All students at IITK?
- Any human in the world?
- Any intelligent being in the universe?

Do we know about our population of interest?

**Random Sample**
A procedure in which every member of the population has the same chance of being selected is called a simple random sample. We can say a process has an element of randomness whenever it is possible to repeat the process and get different answers each time.

If we can’t observe the same thing twice, the observations are said to have been sampled **without replacement**, else it is called sample **with replacement**.

Also recall the defiirent kind of **biases** that can occur during sampling.

**Types of sampling**

  - *Stratified sampling*: Suppose your population is (or can be) divided into several different subpopulations, or _strata_. For eg. you’re running a study at several different sites. Instead of trying to sample randomly from the population as a whole, you instead try to collect a separate random sample from each of the strata. An example would be two equally sample people from schizophrenic and non-schizophrenic populations while studying about schizophrenia. This is also referred to as _oversampling_ because it makes a deliberate attempt to over-represent rare groups.
  - *Snowball sampling*: It is especially useful when sampling from a “hidden” or hard to access population. For instance, suppose the researchers want to conduct an opinion poll among transgender people. For first round, they'll contact the people they know, for second one they'll get contacts of other people from the first set of participants and so on. Despite the advantage that we can get data in situations that might otherwise be impossible to get any, it makes the sample highly non-random and might also be unethical if not handled properly. This is because social networks are complex things, and just because we can use them to get data doesn’t always mean we should.
  - *Convenience sampling*: The samples are chosen in a way that is convenient to the researcher, and not selected at random from the population of interest. Eg. Snowball sampling or when undergraduates or all students if an institute are recruited for a study. 
  
  Can you think of other examples of convenience sampling in biology or psychology?
  
But are such sampling methods or biases wrong?

A bias in your sampling method is only a problem if it causes you to draw the wrong conclusions. We probably need it to be random only with respect to the psychologically/biologically-relevant phenomenon of interest.

##### Population parameters and sample statistics

To understand the relation between population parameters and sample statistics, let's take an example.

We want tos tudy the IQ of people. The population of interest is a group of actual humans who have IQ scores. Let's assume IQ tests are designed so that the average IQ is 100, the standard deviation of IQ scores is 15, and the distribution of IQ scores is normal. These values are referred to as the population parameters because they are characteristics of the entire population. In other words, population mean μ is 100, and the population standard deviation σ is 15.

Now imagine, we are running an experiment to estimate these population characteristics for which we will sample the population. 

```{r}
#Let's sample 100 IQ scores randomly from a population (assuming it has mean of 100, and sd of 15)
IQ <- rnorm(n = 100, mean = 100, sd = 15) # sample IQ scores (generate using rnorm)
IQ <- round(IQ) # IQs are whole numbers!
print(IQ) 
```

```{r}
#Let's also plot and see how it looks like
hist(IQ)
```

```{r}
#Find the mean of this sample
mean(IQ)
sd(IQ)
```

These are *sample statistics* which are properties of our data set, and although they are fairly similar to the true population values, they are not the same. 

What if we collect data from a large number of people?

**Law of large numbers**
```{r}
#Let's sample IQ from 10, 000 people
IQ <- rnorm(n = 10000, mean = 100, sd = 15) # sample IQ scores (generate using rnorm)
IQ <- round(IQ) # IQs are whole numbers!
```


```{r}
#Finding the mean and sd of this sample
mean(IQ)
sd(IQ)
```


```{r}
#Let's also plot a histogram
hist(IQ)
```

Even a mere inspection makes it clear that the larger sample is a much better approximation to the true population distribution than the smaller one. This is reflected in the sample statistics: the mean IQ for the larger sample turns out to be 99.94, and the standard deviation is 15.02. These values are now very close to the true population.

When applied to the sample mean, what the *law of large numbers* states is that as the sample gets larger, the sample mean tends to get closer to the true population mean. Or, to say it a little bit more precisely, as the sample size “approaches” infinity (written as N -> $\infty$) the sample mean approaches the population mean ($\bar{X}$ -> $\mu$).

But is sampling a large number of observations practically possible?

##### Central Limit Theorem

**Sampling distribution of mean**

Let's we can sample only 5 people at a time for their IQ.

```{r}
IQ.1 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.1
```

```{r}
# Finding the mean and sd of this sample
mean(IQ.1)
sd(IQ.1)
```

This is ofcourse much less accurat ethan our previous experiments with 100 and 10,000 samples. 

What if we decide to replicate this experiment? We will sample 5 people once again and find their IQs.

```{r}
IQ.2 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.2
```

Let's say we do 10 such replications

```{r}
IQ.3 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.4 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.5 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.6 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.7 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.8 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.9 <- round(rnorm(n=5, mean = 100, sd = 15))
IQ.10 <- round(rnorm(n=5, mean = 100, sd = 15))
```

If we replicate an experiment over and over again, what we end up with is a distribution of sample means! This distribution has a special name in statistics: it’s called the **sampling distribution of the mean**.

```{r}
mean_dist <- c(mean(IQ.1), 
               mean(IQ.2), 
               mean(IQ.3),
               mean(IQ.4),
               mean(IQ.5),
               mean(IQ.6),
               mean(IQ.7),
               mean(IQ.8),
               mean(IQ.9),
               mean(IQ.10))
hist(mean_dist) #Plotting the sampling distribution of the mean
```
```{r}
#Let's assume we replicated our sample 10,000 times. 
means <- 0 #Initializing vector
for (i in 1:10000) {
  IQ.n <- round(rnorm(n=5, mean = 100, sd = 15))
  means[i] <- mean(IQ.n)
}
hist(means) #Plotting the sampling distribution of the mean
```


```{r}
#Let's assume we replicated our sample a very large number of times. Warning: This code chunk might take a few minutes to run
means <- 0 #Initializing vector
for (i in 1:100000000) {
  IQ.n <- round(rnorm(n=5, mean = 100, sd = 15))
  means[i] <- mean(IQ.n)
}
hist(means) #Plotting the sampling distribution of the mean
```
A sampling distribution can exist for any sample statistic. 
 
For eg., think about the a sampling distribution for the largest IQ score i.e. the _maximum_ of a sample.
 
If you observe carefully, then you might have noticed the difference in the sampling distributions when sample size was 10 as compared to when sample size was 10,000. Try playing around with the sample size in the for loop above and notice the difference in plot.
 
**Central Limit Theorem**
 
The bigger the sample size, the narrower the sampling distribution gets. We can quantify this effect by calculating the standard deviation of the sampling distribution, which is referred to as the *standard error*.
The standard error of a statistic is often denoted SE, and since we’re usually interested in the standard error of the sample mean, we often use the acronym SEM. The sample size is denoted by N.

But what if the population distribution isn’t normal? What happens to the sampling distribution of the mean? 

The remarkable thing is this: no matter what shape your population distribution is, as N increases the sampling distribution of the mean starts to look more like a normal distribution.

Refer to _Section 10.3.3, Navarro D._ for further details.

It seems like we have evidence for all of the following claims about the sampling distribution of the mean:
 - The mean of the sampling distribution is the same as the mean of the population
 - The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the
sample size increases
 - The shape of the sampling distribution becomes normal as the sample size increases
 
The **Central Limit Theorem** proves all of the above statements. It also tells that if the population distribution has mean $\mu$ and standard deviation $\sigma$, then the sampling distribution of the mean also has mean $\mu$, and the standard error of the mean is $$SEM = \frac{\sigma}{\sqrt{N}}$$

#### Estimating population parameters

So far we assumed that the population mean is 100. How do we know that IQ scores have a true population mean of 100? 

Of course almost every research project of interest involves looking at a different population of people to those used in the test norms and so we’re going to have to estimate the population parameters from a sample of data. But how do we do this?

##### Estimating population mean

Let's say we want to estimate the IQ of people of Kanpur. We go to the Blue World water park and the tourists there are kind enough to participate in our study. Say, we get a mean $$\bar{X} = 98.5$$ for 100 locals.

But what is the true mean IQ for the entire population of Kanpur?

Here we can only calculate the *sample mean*, and use that as our estimate of the *population mean*. If true population mean is denoted $\mu$, then we would use $\hat{\mu}$ to refer to our estimate of the population mean. The sample mean is denoted $\bar{X}$ or sometimes _m_.

##### Estimating population standard deviation

What shall we use as our estimate for the standard deviation? Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample statistic as our estimate. That’s almost the right thing to do, but not quite.

Let's again think about the IQ experiment where we had assumed that population mean is 100 and standard deviation is 15. Say we only observe the IQ of 2 people and find the sample standard deviation. 

Let's plot the sampling distribution of the standard deviation.

```{r}
#Replicating 50 times 
sd_dist <- 0 #Initializing vector
for (i in 1:50) {
  IQ.n <- round(rnorm(n=2, mean = 100, sd = 15))
  sd_dist[i] <- sd(IQ.n)
}
hist(sd_dist) #Plotting the sampling distribution of the mean
```

If we take a look at the average of this distribution, we can clearly see it is quite far from 15 whereas if we find the average of the sampling distribution of mean of population, it is quite close to the popuation mean. Try it out your self.

```{r}
mean(sd_dist)
#Find the average for sampling distribution of population mean calculated above
```

On average, the average sample mean is equal to the population mean. It is an _unbiased estimator_, which is essentially the reason why the best estimate for the population mean is the sample mean. The plot on the right is quite different: on average, the sample standard deviation s is smaller than the population standard deviation σ. It is a _biased estimator_. In other words, if we want to make a “best guess” σˆ about the value of the population standard deviation σ, we should make sure our guess is a little bit larger than the sample standard deviation s.






